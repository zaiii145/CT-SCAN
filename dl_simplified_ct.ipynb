{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b11126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL as pl\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AvgPool2D, MaxPooling2D, Flatten, Dense, Dropout, RandomBrightness, RandomFlip\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48156f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m img = cv2.imread(\u001b[33m\"\u001b[39m\u001b[33mfiles/aneurysm/10.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33maneurysm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maneurysm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CT SCAN\\ct_env\\Lib\\site-packages\\matplotlib\\pyplot.py:3601\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3579\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3581\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3599\u001b[39m     **kwargs,\n\u001b[32m   3600\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3613\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3617\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3619\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3621\u001b[39m     sci(__ret)\n\u001b[32m   3622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CT SCAN\\ct_env\\Lib\\site-packages\\matplotlib\\__init__.py:1524\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1521\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1530\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1531\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CT SCAN\\ct_env\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5982\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5980\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5982\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5983\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5985\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CT SCAN\\ct_env\\Lib\\site-packages\\matplotlib\\image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CT SCAN\\ct_env\\Lib\\site-packages\\matplotlib\\image.py:648\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    646\u001b[39m A = cbook.safe_masked_invalid(A, copy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.dtype != np.uint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.can_cast(A.dtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msame_kind\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconverted to float\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A.shape[-\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    651\u001b[39m     A = A.squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(\"files/aneurysm/10.jpg\")\n",
    "plt.title('aneurysm')\n",
    "plt.imshow(img, label = 'aneurysm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/kaggle/input/gssoc-ct-augumented-ds/files/cancer/20.jpg\")\n",
    "plt.title('cancer')\n",
    "plt.imshow(img, label = 'cancer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/kaggle/input/gssoc-ct-augumented-ds/files/tumor/10.jpg\")\n",
    "plt.title('Tumor')\n",
    "plt.imshow(img, label = 'Tumor Case')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    \"\"\"\n",
    "    Load an image from file.\n",
    "\n",
    "    Args:\n",
    "    - file_path: Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - img: Loaded image as a numpy array.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(file_path)\n",
    "    return img\n",
    "\n",
    "def display_sample_images(image_paths, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Display sample images from the dataset.\n",
    "\n",
    "    Args:\n",
    "    - image_paths: List of paths to image files.\n",
    "    - labels: List of corresponding labels for the images.\n",
    "    - num_samples: Number of sample images to display (default: 5).\n",
    "    \"\"\"\n",
    "    buff = 0\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    for i in range(5):\n",
    "        img_path = os.path.join('/kaggle/input/gssoc-ct-augumented-ds/files', image_paths[i+buff])\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            # print(\"Image Loaded Successfully.\")\n",
    "            # print(\"Image dtype:\", img.dtype)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(labels[i+buff])\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            print(\"Error: Unable to load image.\")\n",
    "        buff += 50\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(labels):\n",
    "    \"\"\"\n",
    "    Plot the distribution of classes in the dataset.\n",
    "\n",
    "    Args:\n",
    "    - labels: List of class labels.\n",
    "    \"\"\"\n",
    "    class_counts = pd.Series(labels).value_counts()\n",
    "    class_counts.plot(kind='bar', rot=0)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Class Distribution')\n",
    "\n",
    "    # Add counts over bars\n",
    "    for i, count in enumerate(class_counts):\n",
    "        plt.text(i, count + 0.1, str(count), ha='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def visualize_image_statistics(images):\n",
    "    \"\"\"\n",
    "    Visualize statistics of image data.\n",
    "\n",
    "    Args:\n",
    "    - images: List of image arrays.\n",
    "    \"\"\"\n",
    "    image_shapes = [img.shape for img in images]\n",
    "    image_means = [np.mean(img) for img in images]\n",
    "    image_stds = [np.std(img) for img in images]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].hist(image_shapes)\n",
    "    axes[0].set_title('Image Shapes')\n",
    "    axes[0].set_xlabel('Shape')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    axes[1].hist(image_means)\n",
    "    axes[1].set_title('Image Pixel Mean')\n",
    "    axes[1].set_xlabel('Mean')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    axes[2].hist(image_stds)\n",
    "    axes[2].set_title('Image Pixel Standard Deviation')\n",
    "    axes[2].set_xlabel('Standard Deviation')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def perform_eda(dataset_folder):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis (EDA) on the image dataset.\n",
    "\n",
    "    Args:\n",
    "    - dataset_folder: Path to the dataset folder containing image files.\n",
    "    \"\"\"\n",
    "    # Load dataset information from .csv file\n",
    "    dataset_info = pd.read_csv(os.path.join(dataset_folder, 'ct-brains-augmented.csv'))\n",
    "\n",
    "    # Display sample images\n",
    "    display_sample_images(dataset_info['jpg'], dataset_info['type'])\n",
    "\n",
    "    # Plot class distribution\n",
    "    plot_class_distribution(dataset_info['type'])\n",
    "\n",
    "    # Load images and visualize image statistics\n",
    "    images = []\n",
    "    # print(dataset_info['jpg'])\n",
    "    for file_path in dataset_info['jpg']:\n",
    "        img = load_image(os.path.join('/kaggle/input/gssoc-ct-augumented-ds/files', file_path))\n",
    "        images.append(img)\n",
    "    visualize_image_statistics(images)\n",
    "\n",
    "# Example usage:\n",
    "dataset_folder = r\"/kaggle/input/gssoc-ct-augumented-ds/files\"\n",
    "perform_eda(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10513b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "def define_paths(dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    classes = os.listdir(dir)\n",
    "    for class_name in classes:\n",
    "        if os.path.isdir(os.path.join(dir, class_name)):\n",
    "            class_path = os.path.join(dir, class_name)\n",
    "            filelist = os.listdir(class_path)\n",
    "            for file in filelist:\n",
    "                fpath = os.path.join(class_path, file)\n",
    "                if fpath.endswith('.jpg'):\n",
    "                    filepaths.append(fpath)\n",
    "                    labels.append(class_name)\n",
    "    return filepaths, labels\n",
    "\n",
    "\n",
    "# Concatenate data paths with labels into one dataframe\n",
    "def define_df(files, classes):\n",
    "    Fseries = pd.Series(files, name='filepaths')\n",
    "    Lseries = pd.Series(classes, name='labels')\n",
    "    return pd.concat([Fseries, Lseries], axis=1)\n",
    "\n",
    "# Split data into train, validation, and test\n",
    "def split_data(dir):\n",
    "    files, classes = define_paths(dir)\n",
    "    df = define_df(files, classes)\n",
    "    strat = df['labels']\n",
    "    train_df, valid_df, test_df = train_valid_test_split(df, train_size=0.6, valid_size=0.05, test_size=0.35, shuffle=True, random_state=123, stratify=strat)\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# Custom train, validation, test split function\n",
    "def train_valid_test_split(df, train_size, valid_size, test_size, shuffle=True, random_state=None, stratify=None):\n",
    "    assert train_size + valid_size + test_size == 1.0, \"The sum of train_size, valid_size, and test_size should be 1.0\"\n",
    "\n",
    "\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    num_samples = len(df)\n",
    "    train_end = int(train_size * num_samples)\n",
    "    valid_end = int((train_size + valid_size) * num_samples)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    valid_df = df[train_end:valid_end]\n",
    "    test_df = df[valid_end:]\n",
    "\n",
    "#     return train_df, test_df\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "# Get Dataframes\n",
    "data_dir = r\"/kaggle/input/gssoc-ct-augumented-ds/files\"\n",
    "# train_df, test_df = split_data(data_dir)\n",
    "train_df, valid_df, test_df = split_data(data_dir)\n",
    "\n",
    "# Display the head of each dataframe\n",
    "print(\"Train Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\")\n",
    "print(train_df.tail())\n",
    "print(\"\")\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(valid_df.head())\n",
    "print(\"\")\n",
    "print(valid_df.tail())\n",
    "print(\"\")\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(test_df.head())\n",
    "print(\"\")\n",
    "print(test_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0af070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b00f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_data (train_df, valid_df, test_df, batch_size):\n",
    "\n",
    "    ''' This function takes train, validation, and test dataframe and fit them into image data generator, because model takes\n",
    "         data from image data generator.\n",
    "         Image data generator converts images into tensors. '''\n",
    "\n",
    "\n",
    "    # define model parameters\n",
    "    img_size = (224, 224)\n",
    "    channels = 3 # either BGR or Grayscale\n",
    "    color = 'rgb'\n",
    "    img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "    test_steps = ts_length // test_batch_size\n",
    "\n",
    "    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "    def scalar(img):\n",
    "        return img\n",
    "\n",
    "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
    "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "    \n",
    "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                            color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                                color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "    \n",
    "    # Note: we will use custom test_batch_size, and make shuffle= false\n",
    "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                           color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
    "\n",
    "#     return train_gen, test_gen\n",
    "    return train_gen, valid_gen, test_gen\n",
    "Batch_size = 32\n",
    "train_gen, valid_gen, test_gen = create_model_data(train_df, valid_df, test_df, Batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(gen):\n",
    "\n",
    "    # return classes , images to be displayed\n",
    "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
    "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
    "    images, labels = next(gen)        # get a batch size samples from the generator\n",
    "\n",
    "\n",
    "    # calculate number of dispalyed samples\n",
    "    length = len(labels)        # length of batch size\n",
    "    sample = min(length, 25)    # check if sample less than 25 images\n",
    "\n",
    "    plt.figure(figsize= (20, 20))\n",
    "\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255       # scales data to range (0 - 255)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        # note\n",
    "        index = np.argmax(labels[i])  # get image index\n",
    "\n",
    "        class_name = classes[index]   # get class of image\n",
    "\n",
    "        plt.title(class_name, color= 'blue', fontsize= 8)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3bc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Image Samples\n",
    "show_images(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b705c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, Reshape, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the VGG16 model without the top (classification) layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3), pooling = 'max')\n",
    "\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "# base_model.trainable = False\n",
    "\n",
    "# Create a new model that includes the base_model\n",
    "input_shape = (224, 224, 3)\n",
    "vgg_model= tf.keras.Sequential([\n",
    "#     tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape = (224,224, 3)),\n",
    "#     tf.keras.layers.experimental.preprocessing.Resizing(224, 224),\n",
    "    \n",
    "    base_model,\n",
    "    \n",
    "    Dense(124, activation='relu', kernel_regularizer=L1L2(l1=0.001, l2=0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.45),\n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=L1L2(l1=0.001, l2=0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.45),\n",
    "\n",
    "    Dense(16, activation='relu', kernel_regularizer=L1L2(l1=0.001, l2=0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.45),\n",
    "\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "vgg_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(0.001),\n",
    "    metrics=['accuracy', Precision(), Recall(), AUC()]\n",
    ")\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # set batch size for training\n",
    "epochs = 90     # number of all epochs in training\n",
    "patience = 1 \t\t # number of epochs to wait to adjust lr if monitored value does not improve\n",
    "stop_patience = 3 \t# number of epochs to wait before stopping training if monitored value does not improve\n",
    "threshold = 0.9 \t  # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\n",
    "factor = 0.5 \t\t    # factor to reduce lr by\n",
    "ask_epoch = 5\t\t    # number of epochs to run before asking if you want to halt training\n",
    "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
    "\n",
    "# callbacks = [MyCallback(vgg_model, patience= patience,\n",
    "#                         stop_patience= stop_patience, threshold= threshold, factor= factor,\n",
    "#                         batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vgg_model.fit(\n",
    "    x=train_gen,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=None,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82961c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "# Visualize the model architecture\n",
    "plot_model(vgg_model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22539d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    \n",
    "\n",
    "\n",
    "    plt.figure(figsize= (20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
    "    plt.figure(figsize= (10, 10))\n",
    "    plt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation= 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, Without Normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d290e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = vgg_model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = vgg_model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = vgg_model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870dd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vgg_model.predict(test_gen)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657589d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632c517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b5a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f6007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
